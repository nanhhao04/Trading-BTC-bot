# üìâ Deep Reinforcement Learning for Bitcoin Trading: Mathematical Foundation & Implementation

**Technical Reference for AI Engineers** **Based on:** *Prasetyo et al., 2025* **Topic:** Optimal Control in Non-Stationary Financial Markets (High-Frequency Crypto Trading).

---

## üìë Table of Contents
1. [V·∫•n ƒë·ªÅ C·ªët l√µi: T√≠nh Kh√¥ng D·ª´ng (Non-Stationarity)](#1-v·∫•n-ƒë·ªÅ-c·ªët-l√µi-t√≠nh-kh√¥ng-d·ª´ng-non-stationarity)
2. [Feature Engineering: X·ª≠ l√Ω T√≠n hi·ªáu & Chu·∫©n h√≥a](#2-feature-engineering-x·ª≠-l√Ω-t√≠n-hi·ªáu--chu·∫©n-h√≥a)
3. [Reward Shaping: H√†m M·ª•c Ti√™u ƒêa Bi·∫øn](#3-reward-shaping-h√†m-m·ª•c-ti√™u-ƒëa-bi·∫øn)
4. [Algorithm Dynamics: To√°n h·ªçc c·ªßa DQN vs PPO](#4-algorithm-dynamics-to√°n-h·ªçc-c·ªßa-dqn-vs-ppo)
5. [Implementation Checklist](#5-implementation-checklist)

---

## 1. V·∫•n ƒë·ªÅ C·ªët l√µi: T√≠nh Kh√¥ng D·ª´ng (Non-Stationarity)

Th·ªã tr∆∞·ªùng ti·ªÅn ƒëi·ªán t·ª≠ (Bitcoin) th∆∞·ªùng tu√¢n theo m√¥ h√¨nh **Geometric Brownian Motion** v·ªõi c√°c tham s·ªë thay ƒë·ªïi theo th·ªùi gian (Time-varying parameters):

$$dP_t = \mu_t P_t dt + \sigma_t P_t dW_t$$

* $\mu_t$: Drift (Xu h∆∞·ªõng) thay ƒë·ªïi li√™n t·ª•c.
* $\sigma_t$: Volatility (ƒê·ªô bi·∫øn ƒë·ªông) kh√¥ng ·ªïn ƒë·ªãnh (Heteroscedasticity).
* **H·ªá qu·∫£:** Hi·ªán t∆∞·ª£ng **Covariate Shift**. Ph√¢n ph·ªëi d·ªØ li·ªáu ƒë·∫ßu v√†o $P(X)$ thay ƒë·ªïi li√™n t·ª•c, khi·∫øn m·∫°ng N∆°-ron b·ªã "l·∫°c tr√¥i" (Gradient tr√¥i d·∫°t).

---

## 2. Feature Engineering: X·ª≠ l√Ω T√≠n hi·ªáu & Chu·∫©n h√≥a

ƒê·ªÉ gi·∫£i quy·∫øt Covariate Shift, ta kh√¥ng ƒë∆∞a gi√° th√¥ ($P_t$) v√†o m·∫°ng. Ta c·∫ßn bi·∫øn ƒë·ªïi kh√¥ng gian tr·∫°ng th√°i sang d·∫°ng ·ªïn ƒë·ªãnh (Stationary).

### 2.1. Rolling Z-Score Normalization
K·ªπ thu·∫≠t n√†y ƒë∆∞a ph√¢n ph·ªëi c·ª•c b·ªô t·∫°i m·ªçi th·ªùi ƒëi·ªÉm v·ªÅ chu·∫©n t·∫Øc $\mathcal{N}(0, 1)$.

$$z_t = \frac{x_t - \mu_{rolling}}{\sigma_{rolling}}$$

* **T·∫°i sao c·∫ßn thi·∫øt?**
    * Tr√°nh b√£o h√≤a h√†m k√≠ch ho·∫°t (Activation Saturation). N·∫øu $x_t$ qu√° l·ªõn (gi√° BTC 100k), t√≠ch v√¥ h∆∞·ªõng $W \cdot x + b$ s·∫Ω ƒë·∫©y ƒë·∫ßu ra c·ªßa Sigmoid/Tanh v·ªÅ $\pm 1$, n∆°i ƒë·∫°o h√†m $\approx 0$ (Vanishing Gradient).
    * $z_t$ gi·ªØ cho ƒë·∫ßu v√†o lu√¥n n·∫±m trong v√πng tuy·∫øn t√≠nh c·ªßa h√†m k√≠ch ho·∫°t.

### 2.2. Regime Filter (B·ªô l·ªçc Xu h∆∞·ªõng)
S·ª≠ d·ª•ng ki·∫øn th·ª©c chuy√™n gia (Expert Knowledge) ƒë·ªÉ t√°ch kh√¥ng gian tr·∫°ng th√°i th√†nh 2 ch·∫ø ƒë·ªô (Regimes):

$$I_{trend} = \mathbb{I}(P_t > \text{SMA}_{200})$$

* $I_{trend} = 1$: Bull Market (Uptrend).
* $I_{trend} = 0$: Bear Market (Downtrend).
* **Input Vector $S_t$:**
    $$S_t = [\text{Norm\_Close}, \text{RSI}_{14}, \text{Volatility}, \text{MACD}, \text{SMA\_Dist}, I_{trend}]$$

---

## 3. Reward Shaping: H√†m M·ª•c Ti√™u ƒêa Bi·∫øn

Thay v√¨ d√πng L·ª£i nhu·∫≠n ƒë∆°n thu·∫ßn ($P_t - P_{t-1}$), ta x√¢y d·ª±ng m·ªôt h√†m m·ª•c ti√™u v√¥ h∆∞·ªõng h√≥a (Scalarized Objective) ƒë·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ **Temporal Credit Assignment** v√† **Risk Management**.

**C√¥ng th·ª©c T·ªïng qu√°t:**
$$R_t = \phi_{regime} \times \left( \underbrace{R_{unrealized}}_{\text{D·∫´n h∆∞·ªõng}} + \alpha \underbrace{R_{realized}}_{\text{Ch·ªët ƒë∆°n}} - \beta \underbrace{DD_t}_{\text{R·ªßi ro}} \right) - \text{Cost}$$

### 3.1. Th√†nh ph·∫ßn D·∫´n h∆∞·ªõng (Dense Reward Manifold)
$$R_{unrealized} = \text{Position}_t \times \frac{P_t - P_{t-1}}{P_{t-1}}$$
* T·∫°o ra m·ªôt b·ªÅ m·∫∑t Gradient li√™n t·ª•c, gi√∫p Agent h·ªçc ƒë∆∞·ª£c h∆∞·ªõng ƒëi ƒë√∫ng ngay c·∫£ khi ch∆∞a ƒë√≥ng l·ªánh.

### 3.2. Th√†nh ph·∫ßn R·ªßi ro (Non-linear Penalty)
$$DD_t = \frac{\text{Peak}_t - \text{Equity}_t}{\text{Peak}_t}$$
* Ph·∫°t d·ª±a tr√™n **Max Drawdown**.
* **C∆° ch·∫ø To√°n h·ªçc:** T·∫°o ra m·ªôt "v√°ch n√∫i" (Cliff) trong kh√¥ng gian $Q$-value. N·∫øu m·ªôt h√†nh ƒë·ªông d·∫´n ƒë·∫øn tr·∫°ng th√°i c√≥ Drawdown cao, gi√° tr·ªã $Q(s,a)$ t·∫°i ƒë√≥ s·∫Ω s·ª•t gi·∫£m nghi√™m tr·ªçng $\rightarrow$ Agent h·ªçc c√°ch tr√°nh xa c√°c chu·ªói h√†nh ƒë·ªông r·ªßi ro cao.

### 3.3. H·ªá s·ªë M√¥i tr∆∞·ªùng (Regime Factor)
$$\phi_{regime} = \begin{cases} 1.0 & \text{if } P_t > \text{SMA}_{200} \\ 0.8 & \text{if } P_t \le \text{SMA}_{200} \end{cases}$$
* Gi·∫£m k·ª≥ v·ªçng ph·∫ßn th∆∞·ªüng trong Downtrend. L√†m gi·∫£m $Q$-value c·ªßa h√†nh ƒë·ªông MUA, khi·∫øn Agent tr·ªü n√™n "th·∫≠n tr·ªçng" (Conservative) h∆°n.

---

## 4. Algorithm Dynamics: To√°n h·ªçc c·ªßa DQN vs PPO

### 4.1. DQN (Deep Q-Network) - Robustness against Noise
DQN chi·∫øn th·∫Øng trong th·ªã tr∆∞·ªùng nhi·ªÖu (Sideways) nh·ªù **Huber Loss**.

$$\mathcal{L}_{Huber}(\delta) = \begin{cases} \frac{1}{2}\delta^2 & \text{if } |\delta| \le \delta_{thresh} \\ \delta_{thresh} (|\delta| - \frac{1}{2}\delta_{thresh}) & \text{otherwise} \end{cases}$$

* **Ph√¢n t√≠ch:**
    * V·ªõi nhi·ªÖu nh·ªè (Normal market): H·ªçc nhanh nh∆∞ h√†m b·∫≠c 2 (MSE).
    * V·ªõi nhi·ªÖu l·ªõn/Outliers (Flash crash): H·ªçc tuy·∫øn t√≠nh (Linear), kh√¥ng b·ªã qu√° nh·∫°y c·∫£m.
* **H√†nh vi:** DQN l·ªçc b·ªè nhi·ªÖu, ch·ªâ v√†o l·ªánh khi t√≠n hi·ªáu c·ª±c r√µ ($Q$-value v∆∞·ª£t tr·ªôi). **Ph√π h·ª£p cho Swing Trading.**

### 4.2. PPO (Proximal Policy Optimization) - Vulnerability in High Variance
PPO g·∫∑p kh√≥ khƒÉn do ∆∞·ªõc l∆∞·ª£ng Advantage ($A_t$) c√≥ ph∆∞∆°ng sai cao.

$$L^{CLIP} = \mathbb{E} [ \min(r_t A_t, \text{clip}(r_t, 1-\epsilon, 1+\epsilon) A_t) ]$$

* **V·∫•n ƒë·ªÅ:**
    * Trong Crypto, $A_t(s,a) = Q(s,a) - V(s)$ bi·∫øn ƒë·ªông c·ª±c m·∫°nh.
    * PPO l√† thu·∫≠t to√°n ng·∫´u nhi√™n (Stochastic Policy). Vi·ªác "th·ª≠ sai" trong m√¥i tr∆∞·ªùng bi·∫øn ƒë·ªông l·ªõn d·∫´n ƒë·∫øn chu·ªói thua l·ªó li√™n ti·∫øp (Whipsaw effect).
* **H√†nh vi:** PPO trade r·∫•t nhi·ªÅu, th·∫Øng l·ªõn khi c√≥ Trend m·∫°nh, nh∆∞ng thua l·ªó n·∫∑ng khi th·ªã tr∆∞·ªùng ƒë·∫£o chi·ªÅu (Mean Reversion). **Ph√π h·ª£p cho Momentum Trading.**

---

## 5. Implementation Checklist

ƒê·ªÉ t√°i hi·ªán (reproduce) th√†nh c√¥ng b√†i b√°o n√†y, Codebase c·∫ßn ƒë·∫£m b·∫£o:

- [ ] **Preprocessing:**
    - [ ] C√†i ƒë·∫∑t `RollingWindow` ƒë·ªÉ t√≠nh Z-Score (kh√¥ng d√πng Global Mean).
    - [ ] T√≠nh `SMA200` ƒë·ªÉ t·∫°o feature `trend_flag`.
- [ ] **Environment (Gym):**
    - [ ] `step()` function ph·∫£i tr·∫£ v·ªÅ `info` ch·ª©a Drawdown hi·ªán t·∫°i.
    - [ ] Implement h√†m reward t·ªïng h·ª£p (Unrealized + Realized - Penalty).
- [ ] **Agent Config (Stable-Baselines3):**
    - [ ] **DQN:** S·ª≠ d·ª•ng `exploration_fraction=0.1` (gi·∫£m explore khi deploy), `huber_loss`.
    - [ ] **PPO:** TƒÉng `ent_coef` (Entropy coefficient) n·∫øu Agent b·ªã k·∫πt (kh√¥ng ch·ªãu trade), ho·∫∑c gi·∫£m n·∫øu trade qu√° nhi·ªÅu.
- [ ] **Safety Layer:**
    - [ ] Hard-code quy t·∫Øc: N·∫øu `Close < SMA200`, gi·∫£m `max_position_size` xu·ªëng 50%.

---
*Document prepared by Gemini specifically for AI Engineering usage.*